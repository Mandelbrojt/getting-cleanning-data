---
title: "Getting and Cleaning Data"
author: "Luis Moreno"
date: "9/21/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Note: the `echo = FALSE` parameter is added to the code chunk to prevent printing of the R code.  

## Reading Excel files  
```{r, eval = FALSE}
fileURL <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.xlsx?accessType=DOWNLOAD"
download.file(fileURL, destfile = "./data/cameras.xlsx", method = "curl")
dateDownloaded <- date()
```

### read.xlsx()  
This function allows the user to read data from an Excel worksheet. It provides the conveniency of read.table by borrowing from its signature.  

### Main arguments for read.xlsx function  
```{reading_xlsx, eval = FALSE}
library(xlsx)
str(read.xlsx)
```

**file**: the path to the file to read.  
**sheetIndex**: a number representing the sheet index in the workbook.  
**sheetName**: a character string with the sheet name.  
**rowIndex**: a numeric vector indicating the rows you want to extract.  
**colIndex**: a numeric vector indicating the cols you want to extract.  
**head**: a logical value indicating whether the first row corresponding to the first element of the *rowIndex* vector contains the names of the variables.  

```{r, eval = FALSE}
colIndex <- 2:3
rowIndex <- 1:4
cameraDataSubset <- read.xlsx("./data/cameras.xlsx", sheetIndex = 1,
                              colIndex = colIndex, rowIndex = rowIndex)
cameraDataSubset
```

### Further Notes  
**write.xlsx**: it will write out an Excel file with similar arguments.  
**read.xlsx2**: it is faster than read.xsls but for reading subsets of rows is more unstable.  
**XLConnect**: this package has more options for writing and manipulating Excel files.  
**XLConnect vignette**: is a good place to start for that package.  
**Advise**: it is better to store data into a database or in comma separated files (.csv) or tab separated files (.tab/.txt).  

## Reading XML files  
### XML  
XML is the abbreviation for "Extensible Markup Language". It is frequently used to store *structured data*, particularly in internet applications.  
Its components are:  
 - Markup: labels that give the text structure.  
 - Content: the actual text of the document.  
 
 [XML Tutorial](https://www.w3schools.com/xml/)   
 
 ### Tags, elements and attributes  
  - Start tags: <section>  
  - End tags: </section>  
  - Empty tags: <line-break>  
  - Attributes: <step number="3"> Connect A to B. </step>  

```{Reading XML, eval = FALSE}
library(XML)
fileURL <- "./data/yuri_piquet.xml"
doc <- xmlTreeParse(fileURL, useInternalNodes = TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
names(rootNode)
```

Setting "useInternalNodes" to TRUE, allows us to obtain all internal nodes from the XML file.  

### Accessing parts of the XML document with square brackets
```{r, eval = FALSE}
### Accessing parts of the XML document with square brackets
rootNode[[1]]
rootNode[[1]][[1]]
rootNode[[1]][[1]][[1]][[1]]
```

### Programatically extract objects from an XML file
If your variable (in this example the "rootNode") contains the entire XML, then this function will go through every single tagged element in the entire document. Generally, it will return all the text inside the XML file.  

```{r, eval = FALSE}
## Extract all text from the XML file
xmlSApply(rootNode, xmlValue)
```

### XPath Language  
- /node: returns the top level node.  
- //node: returns a node at any level.    
- node[@attr-name]: returns a node with an attribute name.  
- node[@attr-name='bob']: returns a node with an attribute name 'bob'.  

```{r, eval = FALSE}
## Extract all values from an specific node
xpathSApply(rootNode, "//SaldoActual", xmlValue)
xpathSApply(doc, "//NombreOtorgante", xmlValue)
xpathSApply(doc, "//CreditoMaximo", xmlValue)
```

## Reading HTML source code  
```{r, eval = FALSE}
fileURL <- "https://www.espn.com/nfl/team/_/name/bal/baltimore-ravens"
doc <- htmlTreeParse(fileURL, useInternal = TRUE)
## Look for elements that are list items that have a particular class
scores <- xpathSApply(doc, "//li[@class='score']", xmlValue)
teams <- xpathSApply(doc, "//li[@class='team-name']", xmlValue)
```

## Reading JSON  
JSON stands for "Javascript Object Notation", ant it is used for lightweight data storage. It is similar in structure to XML files but different syntax/format.  
Data on JSON can be stored as:  
- Numbers  
- Strings  
- Boolean  
- Array  
- Object  

[More on JSON](https://en.wikipedia.org/wiki/JSON)  

### Reading data with jsonlite package  
```{r, eval = FALSE}
library(jsonlite)
## Read a JSON file for APPLE stock candle quotes from Finnhub
jsonData <- fromJSON("https://finnhub.io/api/v1/stock/candle?symbol=AAPL&resolution=1&from=1572651390&to=1572910590&token=br08q97rh5r9j5ovs4fg")
## Obtain the names in the JSON file
names(jsonData)
## Obtain nested objects in JSON
names(jsonData$c)
```

### Writing data frames into JSON  
```{r, eval = FALSE}
## Convert a data frame in R into JSON format
myjson <- toJSON(iris, pretty = TRUE)
cat(myjson)
## Convert back to JSON
iris2 <- fromJSON(myjson)
head(iris2)
```

### Further resources  
[JSON Official Website](https://www.json.org/json-en.html)  
[JSON Tutorial](https://www.r-bloggers.com/2013/12/new-package-jsonlite-a-smarter-json-encoderdecoder/)  

## The data.table Package  
All functions that accept data.frame can be applied over data.table.  
This package is written in C, making it much faster for subsetting, grouping, updating, among other.  
```{r, eval = FALSE}
library(data.table)
## Create a data frame using data.frame method
DF = data.frame(x = rnorm(9), y = rep(c("a", "b", "c"), each = 3), z = rnorm(9))
head(DF, 3)

## Create a data table using data.table method
DF = data.table(x = rnorm(9), y = rep(c("a", "b", "c"), each = 3), z = rnorm(9))
head(DF, 3)

## See all the data tables in memory
tables()

## Subsetting rows
DT[2, ]
DT[DT$y='a', ]
DT[c(2,3)]
```

### Subsetting in data.table  
- Subsetting function is modified for data.table.  
- The argument after the comma is called an "expression".  
- In R, an expression is a collection of statements enclosed in curly brackets.  

### Calculating values for variables with expressions  
```{r, eval = FALSE}
## Subsetting columns
DT[, c(2,3)]

#Subset the data table applying functions
DT[, list(mean(x), sum(z))]
DT[, table(y)]

## Add new columns
DT[, w:=z^2]

## Multiple step operations are combine with an expression that starts and ends with curly brackets, each statement is followed by a semicolon.
DT[, m:= {tmp <- (x+z); log2(tmp+5)}]

## plyr like operations
# Take the mean of x + w and group it by variable "a"
DT[, b:= mean(x+w), by = a]

## Special variables
# .N is an integer of length 1 cointaining the number
set.seed(123);
DT <- data.table(x = sample(letters[1:3], 1E5, TRUE))
## Count the number of times a value of column x appears
DT[, .N, by = x]
```

### Keys  
```{r, eval = FALSE}
## Set the key of the table to the column x
DT <- data.table(x = rep(c("a", "b", "c"), each = 100), y = rnorm(300))
setkey(DT, x)
DT['a']

## Join two tables by setting the keys of both
DT1 <- data.table(x = c('a', 'a', 'b', 'dt1'), y = 1:4)
DT2 <- data.table(x = c('a', 'a', 'b', 'dt2'), z = 5:7)
setkey(DT1, x); setkey(DT2, x)
merge(DT1, DT2)

## Fast reading of table files
big_df <- data.frame(x = rnorm(1E6), y = rnorm(1E6))
file <- tempfile()
write.table(big_df, file = file, row.names = FALSE, col.names = TRUE, sep = "\t", quote = FALSE)
system.time(fread(file))

system.time(read.table(file, header = TRUE, sep = "\t"))
```

### Further Reading  
[Latest Developments on Data Tables](https://r-forge.r-project.org/scm/viewvc.php/pkg/NEWS?view=markup&root=datatable)   
[Differences Between data.table and data.frame](http://stackoverflow.com/questions/13618488/what-you-can-do-with-data-frame-that-you-cant-in-data-table)  
[Notes on data.tables](https://github.com/raphg/Biostat-578/blob/master/Advanced_data_manipulation.Rmd)  

## Week 1 Quiz  
### Question 1  

The American Community Survey distributes downloadable data about United States communities. Download the 2006 microdata survey about housing for the state of Idaho using download.file() from here:   https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv  
How many properties are worth $1,000,000 or more?  
```{r, eval = FALSE}
library(readr)
us_comminities <- "./data/us_communities.csv"
df <- data.frame(read_csv(us_comminities))
VAL <- df$VAL
VAL <- na.omit(df$VAL)length(mio_val)
mio_val <- subset(VAL, VAL == "24")
length(mio_val)
```

### Question 2  
Use the data you loaded from Question 1. Consider the variable FES in the code book. Which of the "tidy data" principles does this variable violate?   
**Answer**: Each variable in the tidy data set has been transformed to be interpretable.  

### Question 3  
Download the Excel spreadsheet on Natural Gas Aquisition   Program here: https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx  
Read rows 18-23 and columns 7-15 into R and assign the result to a variable called "dat".    
What is the value of: sum(dat$Zip*dat$Ext,na.rm=T)  
```{r, eval = FALSE}
library(readxl)
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
download.file(fileURL, destfile = "./data/nat_gas_adq.xlsx")
xlsx_file_path <- "./data/nat_gas_adq.xlsx"
dat <- read_excel(xlsx_file_path, range = "G18:O23")
sum(dat$Zip*dat$Ext, na.rm = TRUE)
```

### Question 4   
Read the XML data on Baltimore restaurants from here:   https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml  
How many restaurants have zipcode 21231?  
```{r, eval = FALSE}
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
download.file(fileURL, destfile = "./data/baltimore_restaurants.xml")
library(XML)
file_path <- "./data/baltimore_restaurants.xml"
doc <- xmlTreeParse(file_path, useInternalNodes = TRUE)
rootNode <- xmlRoot(doc)
zip_codes <- xpathSApply(rootNode, "//zipcode", xmlValue)
length(subset(zip_codes, zip_codes == "21231"))
```

### Question 5  
The American Community Survey distributes downloadable data about United States communities. Download the 2006 microdata survey about housing for the state of Idaho using download.file() from here:  
https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv  
using the fread() command load the data into an R object "DT" The following are ways to calculate the average value of the variable "pwgtp15" broken down by sex.  
Using the data.table package, which will deliver the fastest user time?    

```{r, eval = TRUE}
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
library(readr)
library(data.table)
file_path <- "./data/american_comm_survey.csv"
DT = fread(file_path)
system.time(mean(DT$pwgtp15, by = DT$SEX))
system.time(tapply(DT$pwgtp15, DT$SEX, mean))
system.time(DT[, mean(pwgtp15), by = SEX])
system.time(sapply(split(DT$pwgtp15, DT$SEX), mean))
system.time(mean(DT[DT$SEX == 1, ]$pwgtp15)) + system.time(mean(DT[DT$SEX == 2, ]$pwgtp15))

system.time(tapply(DT$pwgtp15,DT$SEX,mean))
system.time(mean(DT[DT$SEX==1,]$pwgtp15), mean(DT[DT$SEX==2,]$pwgtp15))
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
system.time(DT[,mean(pwgtp15),by=SEX])
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
```
**ANSWER: DT[,mean(pwgtp15), by=SEX]**  

## Reading from MySQL  
Install RMySQL 
If using Mac: install.packages("RMySQL")  
If using Windows:  visit **http://biostat.mc.vanderbilt.edu/wiki/Main/RMySQL**, or  **http://www.ahschulz.de/2013/07/23/installing-rmysql-under-windows/**  
### UCSC Database  
Database Help Page: **http://genome.ucsc.edu/goldenPath/help/mysql.html**  
### Connecting and listing databases in R  
```{r, eval = FALSE}
library(RMySQL)
ucscDB <- dbConnect(MySQL(), user = "genome",
                    host = "genome-mysql.cse.ucsc.edu")
result <- dbGetQuery(ucscDB, "show databases;"); dbDisconnect(ucscDB);
head(result, 5)
```
### Connecting to hg19 and listing tables  
```{r, eval = FALSE}
hg19 <- dbConnect(MySQL(), user = "genome", db = "hg19",
                  host = "genome-mysql.cse.ucsc.edu")
allTables <- dbListTables(hg19)
length(allTables)
allTables[1:5]
```

### Getting dimensions of a specific table  
```{r, eval = FALSE}
dbListFields(hg19, "affyU133Plus2")
```
### Executing queries
```{r, eval = FALSE}
dbGetQuery(hg19, "select count(*) from affyU133Plus2")
```
### Reading from a table
```{r, eval = FALSE}
affyData <- dbReadTable(hg19, "affyU133Plus2")
head(affyData)
```
### Select a specific subset
```{r, eval = FALSE}
query <- dbSendQuery(hg19, "SELECT * FROM affyU133Plus2 WHERE misMatches BETWEEN 1 AND 3")
affyMis <- fetch(query); quantile(affyMis$misMatches)
affyMisSmall <- fetch(query, n = 10); dbClearResult(query);
dim(affyMisSmall)
dbDisconnect(hg19)
```
NOTE: Every time a connection is done using, it must be closed with the dbDisconnect() function.  

### Further Reading  
RMySQL Vignette: https://cran.r-project.org/web/packages/RMySQL/RMySQL.pdf  
Lis of commands: https://www.pantz.org/software/mysql/mysqlcommands.html  
Other resources: https://www.r-bloggers.com/2011/08/mysql-and-r/  

## HDF5
The Hierarchical Data Format (HDF) is a set of files (HDF4, HDF5) designed to organize and store large amounts of data. 
The rhdf5 package is thus suited for the exchange of large and/or complex datasets between R and other software package, and for letting R applications work on datasets that are larger than the available RAM.  
Groups that contain zero or more data sets and metadata:  
- Have a **group header** with a group name and list attributes.  
- Have a **group symbol table** with a list of objects in group.  
Datasets multidimensional array of data elements with metadata  
- Have a header with name, datatype, dataspace, and storage layout  
- Have a data array with the data  
### HDF5 R Package  
 
```{r, eval = FALSE}
# Install R HDF5 Package
source("http://bioconductor.org/biocLite.R")
biocLite("rhdf5")

# If there is an error with R version try installing with
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install(version = "3.11")

BiocManager::install("rhdf5")

# Load HDF5 Package
library(rhdf5)
created = h5createFile("example.h5")
created

# More information on rhdf5
browseVignettes("rhdf5")
```

### Creating groups in rhdf5
```{r, eval = FALSE}
created = h5createGroup("example.h5", "foo")
created = h5createGroup("example.h5", "baa")
created = h5createGroup("example.h5", "foo/foobaa")
h5ls("example.h5")
```

### Writting groups in rhdf5
```{r,eval = FALSE}
A = matrix(1:10, nrow = 5, ncol = 2)
h5write(A, "example.h5", "foo/A")

B = array(seq(0.1, 2.0, by = 0.1), dim = c(5,2,2))
attr(B, "scale") <- "liter"
h5write(B, "example.h5", "foo/foobaa/B")
h5ls("example.h5")
```

### Writting data sets
```{r,eval = FALSE}
df = data.frame(1L:5L, seq(0, 1, length.out = 5),
                c("ab", "cde", "fghi", "a", "s"), stringsAsFactors = FALSE)
h5write(df, "example.h5", "df")
h5ls("example.h5")
```

### Writting data sets
```{r,eval = FALSE}
readA = h5read("example.h5", "foo/A")
readB = h5read("example.h5", "foo/foobaa/B")
readdf = h5read("example.h5", "df")
readA
```

### Writting and reading chunks
```{r,eval = FALSE}
h5write(c(12, 13, 14), "example.h5", "foo/A", index = list(1:3, 1))
h5read("example.h5", "foo/A")
```

## Reading form the Web  
**Webscraping**: programatically extracting data from the HTML code of websites.  
###Getting data off web pages with readLines()  
```{r, eval = FALSE}
con = url("https://www.tutiempo.net/guadalajara.html?datos=calendario")
htmlCode = readLines(con)
close(con)
htmlCode
```

### Parsing with XML  
```{r, eval = FALSE}
# This code is currently returning an error message
library(XML)
url <- "https://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url, useInternalNodes = TRUE)
xpathSApply(html, "//td", xmlValue)
xpathSApply(html, "//td[@id = 'col-citedby']", xmlValue)
```
### GET from the httr Package  
```{r, eval = FALSE}
library(httr)
url <- "https://www.tutiempo.net/guadalajara.html?datos=calendario"
html2 = GET(url)
content2 = content(html2, as = "text")
library(XML)
parsedHTML <- htmlParse(content2, asText = TRUE)
xpathSApply(parsedHTML, "//title", xmlValue)
xpathSApply(parsedHTML, "//td", xmlValue)
data <- xpathSApply(parsedHTML, "//td", xmlValue)
variables <- data[c(267, 269, 271, 273, 275, 277, 279, 281)]
```

### Accesing websites with passwords  
```{r, eval = FALSE}
pg1 = GET("http://httpbin.org/basic-auth/user/passwd")
pg1
# Accessing into a website with an user and password
pg2 = GET("http://httpbin.org/basic-auth/user/passwd", authenticate("user", "passwd"))
pg2
# Returns the components of the website
names(pg2)
## Returns the contents of the website
content(pg2)
```
### Using handles  
Using handles lets you "save" the authentication across multiple accesses to a website.  
```{r, eval = FALSE}
# Set the Google website as the handler
google = handle("https://www.google.com/")
pag1 = GET(handle = google, path = "/")
pag2 = GET(handle = google, path = "search")
```

## Reading from APIs  
### Accessing Twitter API from R  
```{r, eval = FALSE}
myapp = oauth_app("twitter",
                  key = "H8Kg1tQYXOwZUmSSvyAhNCVyy",
                  secret = "i1XRYLQU5o1o9tYX5xUGCG38fceLPhyDPKw4RN5SLdBlFaoKo0")
sig = sign_oauth1.0(myapp,
                    token = "1311497555662962688-pTlv5FeIHkFsYmc6LqcIOq650eXrEc",
                    token_secret = "RzhEi6XCRQxYonsh6idt9eyceV22wvQAZoZoaH6LjhbqS")
homeTL = GET("https://api.twitter.com/2/users/Trafico_ZMG", sig)
```

### Nothes and further resources  
[**Web Scraping Via XPath**](https://www.r-bloggers.com/2011/11/web-scraping-yahoo-search-page-via-xpath/)  
[**httr Package**](https://cran.r-project.org/web/packages/httr/vignettes/quickstart.html)  

## Getting and Cleaning Data - Week 2 Quiz
### Question 1. 
Instructions: Register an application with the Github API here https://github.com/settings/applications. Access the API to get information on your instructors repositories (hint: this is the url you want "https://api.github.com/users/jtleek/repos").  
Use this data to find the time that the datasharing repo was created.  
What time was it created?
```{r, eval = FALSE}
library(jsonlite)
URL <- "https://api.github.com/users/jtleek/repos"
jsonData <- fromJSON(URL)
names(jsonData)
df <- data.frame(jsonData)
library(dplyr)
ds <- filter(df, name == "datasharing")
names(ds)
ds <- select(ds, c("id", "name", "created_at", "updated_at"))
ds$created_at
```

### Question 2 and 3  
The **sqldf** package allows for execution of SQL commands on R data frames. We will use the sqldf package to practice the queries we might send with the dbSendQuery command in RMySQL.  
Download the American Community Survey data and load it into an R object called "acs"  
Which of the following commands will select only the data for the probability weights pwgtp1 with ages less than 50?  
```{r, eval = FALSE}
# Download the CSV file from the Web
file_URL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(file_URL, destfile = "./data/acs.csv", method = "curl")
# Read the downloaded file as a CSV and convert it to a data frame
file_path <- "./data/acs.csv"
library(readr)
acs <- read_csv(file_path)
acs <- data.frame(acs)
# Install the "sqldf" package, including H2
install.packages("sqldf", dep = TRUE)
# Load the "sqldf" package
library(sqldf)
# Select only the data for the probability weights "pwgtp1" with ages less than 50
sqldf("select pwgtp1 from acs where AGEP < 50")
# What is the equivalent function to "unique(acs$AGEP) in SQL?"
u <- data.frame(unique(acs$AGEP))
class(u)
head(u, 10)
u2 <- sqldf("select distinct AGEP from acs")
class(u2)
head(u2, 10)
head(u == u2, 5)
```
### Question 4  
How many characters are in the 10th, 20th, 30th and 100th lines of HTML from this page: http://biostat.jhsph.edu/~jleek/contact.html  
```{r, eval = FALSE}
con = url("http://biostat.jhsph.edu/~jleek/contact.html")
htmlCode = readLines(con)
close(con)
# Extract characters in the 10th, 20th, 30th and 100th lines
htmlCode[c(10, 20, 30, 100)]
# Count the number of characters of each line
nchar(htmlCode[c(10, 20, 30, 100)])
```
### Question 5  
Read this data set into R and report the sum of the numbers in the fourth of the nine columns.  
https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for  
Original source of the data: http://www.cpc.ncep.noaa.gov/data/indices/wksst8110.for  

(Hint this is a fixed width file format)  
```{r, eval = FALSE}
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for", destfile = "./data/weekly_sst.txt", method = "curl")
library(readr)
file_path <- "./data/weekly_sst.txt"
columns <- c("week", "n1_sst")
df <- read_fwf(file_path, fwf_empty(file_path, col_names = c("week", "n1+2_SST", "n1+2_SSTA", "n3_SST", "n3_SSTA", "n34_SST", "n34_SSTA", "n4_SST", "n4_SSTA")))
```

## Subsetting and Sorting  
### Subsetting with square brackets  
```{r, eval = FALSE}
# Set seed for reproducibility
set.seed(13435)
# Create a data frame with three variables
X <- data.frame("var1" = sample(1:5),
                "var2" = sample(6:10),
                "var3" = sample(7:11))
# Insert NA values in some rows of the data frame
X <- X[sample(1:5), ]; x$var2[c(1,3)] = NA

# Subset the first column by index
X[, 1]
# Subset the first column by column name
X[, "var1"]
# Subset rows of a column by index and column name
X[1:2, "var2"]

```
### Subsetting with logicals and functions  
```{r, eval = FALSE}
# Subset rows of a data frame with "and" logical
X[(X$var1 <= 3 & X$var3 > 11), ]
# Subset rows of a data frame with "or" logical
X[(X$var1 <= 3 | X$var3 > 15), ]
# Subset rows of a data frame with the "which" function
X[which(X$var2 > 8), ]
```
### Sorting  
```{r, eval = FALSE}
# Sort first column in increasing order
sort(X$var1)
# Sort first column in decreasing order
sort(X$var1, decreasing = TRUE)
# Sort NA values of a column at last
sort(X$var2, na.last = TRUE)
```
### Ordering  
```{r, eval = FALSE}
# Order all rows of a data frame by a column
X[order(X$var1), ]
# Order all rows of a data frame by multiple columns
X[order(X$var1, X$var3), ]
```
### Arranging data with plyr package  
```{r, eval = FALSE}
library(plyr)
# Arrange a data frame by a column
arrange(X, var1)
# Arrange a data frame by a column in descending order
arrange(X, desc(var1))
```
### Adding rows and columns  
```{r, eval = FALSE}
# Add a column by varaible specification
X$var4 <- rnorm(5)
# Add a column using "cbind"
Y <- cbind(X, rnorm(5))
# Add a row using "rbinf"
Y <- rbind(X, rnorm(5))
```
## Summarizing Data  
```{r, eval = FALSE}
file <- "https://data.baltimorecity.gov/api/views/k5ry-ef3g/rows.csv?accessType=DOWNLOAD"
restData <- read_csv(file)
# Show the first 5 rows of the CSV file
head(restData, 5)
# Show the last 5 rows of the CSV file
tail(restData, 5)
# Show the overall summary of the CSV file
summary(restData)
# Show the structureof the CSV file
str(restData)
# Show the column names of the CSV file
names(restData)
# Show the quantiles for quantitative data deleting NA values
quantile(restData$councilDistrict, na.rm = TRUE)
# Show the percentiles of a quantitative variable
quantile(restData$councilDistrict, na.rm = TRUE, probs = c(0.5, 0.75, 0.9))
```
### Create Tables  
```{r, eval = FALSE}
# Build a contingency table of the counts of an object as factors counting also NA values
table(restData$zipCode, useNA = "ifany")
# Build a two-dimensional contingency table for qualitative variables
table(restData$zipCode, restData$councilDistrict)
```
### Check for Missing Values  

